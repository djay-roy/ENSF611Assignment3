{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 3: Non-Linear Models and Validation Metrics (37 total marks)\n",
    "### Due: October 24 at 11:59pm\n",
    "\n",
    "### Name: Dhananjay Roy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses non-linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2d2c3",
   "metadata": {},
   "source": [
    "## Part 1: Regression (14.5 marks)\n",
    "\n",
    "For this section, we will be continuing with the concrete example from yellowbrick. You will need to compare these results to the results from the previous assignment. Please use the results from the solution if you were unable to complete Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (0.5 marks)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1030, 8)\n",
      "(1030,)\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import concrete dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_concrete\n",
    "\n",
    "# Load the concrete dataset\n",
    "X, y = load_concrete()\n",
    "\n",
    "# Print the shapes of X and y to confirm the data is loaded\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0 marks)\n",
    "\n",
    "Data processing was completed in the previous assignment. No need to repeat here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import the Decision Tree, Random Forest and Gradient Boosting Machines regression models from sklearn\n",
    "2. Instantiate the three models with `max_depth = 5`. Are there any other parameters that you will need to set?\n",
    "3. Implement each machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(max_depth=5, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=5, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(max_depth=5, random_state=0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)\n",
    "# Instantiate models with max_depth = 5\n",
    "decision_tree = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
    "random_forest = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "gradient_boosting = GradientBoostingRegressor(max_depth=5, random_state=0)\n",
    "\n",
    "# Assuming X, y are your data\n",
    "# Fit the models\n",
    "decision_tree.fit(X, y)\n",
    "random_forest.fit(X, y)\n",
    "gradient_boosting.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the average training and validation accuracy using mean squared error with cross-validation. To do this, you will need to set `scoring='neg_mean_squared_error'` in your `cross_validate` function and negate the results (multiply by -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ad9e365c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "Average Training MSE: 47.823\n",
      "Average Validation MSE: 74.045\n",
      "\n",
      "Random Forest:\n",
      "Average Training MSE: 30.296\n",
      "Average Validation MSE: 47.615\n",
      "\n",
      "Gradient Boosting:\n",
      "Average Training MSE: 3.694\n",
      "Average Validation MSE: 23.547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Ensure that these models and data are defined and split appropriately\n",
    "# decision_tree, random_forest, gradient_boosting, X_train, y_train\n",
    "\n",
    "cv_results_dt = cross_validate(decision_tree, X_train, y_train, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "cv_results_rf = cross_validate(random_forest, X_train, y_train, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "cv_results_gb = cross_validate(gradient_boosting, X_train, y_train, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "train_mse_dt = cv_results_dt['train_score'].mean() * -1\n",
    "validation_mse_dt = cv_results_dt['test_score'].mean() * -1\n",
    "\n",
    "train_mse_rf = cv_results_rf['train_score'].mean() * -1\n",
    "validation_mse_rf = cv_results_rf['test_score'].mean() * -1\n",
    "\n",
    "train_mse_gb = cv_results_gb['train_score'].mean() * -1\n",
    "validation_mse_gb = cv_results_gb['test_score'].mean() * -1\n",
    "\n",
    "print(\"Decision Tree:\")\n",
    "print(f\"Average Training MSE: {train_mse_dt:.3f}\")\n",
    "print(f\"Average Validation MSE: {validation_mse_dt:.3f}\")\n",
    "\n",
    "print(\"\\nRandom Forest:\")\n",
    "print(f\"Average Training MSE: {train_mse_rf:.3f}\")\n",
    "print(f\"Average Validation MSE: {validation_mse_rf:.3f}\")\n",
    "\n",
    "print(\"\\nGradient Boosting:\")\n",
    "print(f\"Average Training MSE: {train_mse_gb:.3f}\")\n",
    "print(f\"Average Validation MSE: {validation_mse_gb:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3f7a8",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: DT, RF and GB\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fdc93a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training MSE Validation MSE\n",
      "DT    47.822974      74.045335\n",
      "RF    30.296363      47.614708\n",
      "GB     3.694308        23.5465\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "# Assuming X_train, y_train are training data\n",
    "# Perform cross-validation\n",
    "models = [decision_tree, random_forest, gradient_boosting]\n",
    "model_names = ['DT', 'RF', 'GB']\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results = pd.DataFrame(columns=['Training MSE', 'Validation MSE'], index=model_names)\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    cv_results = cross_validate(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "    \n",
    "    # Store the results in the DataFrame\n",
    "    results.loc[name, 'Training MSE'] = -1 * cv_results['train_score'].mean()\n",
    "    results.loc[name, 'Validation MSE'] = -1 * cv_results['test_score'].mean()\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31715a9d",
   "metadata": {},
   "source": [
    "Repeat the step above to print the R2 score instead of the mean-squared error. For this case, you can use `scoring='r2'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "83539f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Training R2 Validation R2\n",
      "Decision Tree        0.830437      0.735184\n",
      "Random Forest        0.892634      0.830004\n",
      "Gradient Boosting    0.986903      0.916155\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Instantiate models with max_depth = 5 and random_state = 0\n",
    "decision_tree = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
    "random_forest = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "gradient_boosting = GradientBoostingRegressor(max_depth=5, random_state=0)\n",
    "\n",
    "# Assuming X_train, y_train are training data\n",
    "# Perform cross-validation\n",
    "models = [decision_tree, random_forest, gradient_boosting]\n",
    "model_names = ['Decision Tree', 'Random Forest', 'Gradient Boosting']\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_r2 = pd.DataFrame(columns=['Training R2', 'Validation R2'], index=model_names)\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    cv_results = cross_validate(model, X_train, y_train, cv=5, scoring='r2', return_train_score=True)\n",
    "    \n",
    "    # Store the results in the DataFrame\n",
    "    results_r2.loc[name, 'Training R2'] = cv_results['train_score'].mean()\n",
    "    results_r2.loc[name, 'Validation R2'] = cv_results['test_score'].mean()\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5257a98",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do these results compare to the results using a linear model in the previous assignment? Use values.\n",
    "2. Out of the models you tested, which model would you select for this dataset and why?\n",
    "3. If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03389351",
   "metadata": {},
   "source": [
    "1. Linear vs. Non-linear Model Performance:\n",
    "\n",
    "    Linear Regression Model:\n",
    "\n",
    "    R2 Scores: Training: 0.61, Validation: 0.62\n",
    "    Mean Squared Error (MSE): Training: 111, Validation: 96\n",
    "    Non-linear Models:\n",
    "    All non-linear models demonstrated superior performance compared to the linear model in terms of both R2 and MSE scores.\n",
    "\n",
    "    Gradient Boosting Model:\n",
    "\n",
    "    R2 Scores: Training: 0.988, Validation: 0.919\n",
    "    MSE: Training: 3.379, Validation: 22.783\n",
    "    Random Forest Model:\n",
    "\n",
    "    R2 Scores: Training: 0.897, Validation: 0.841\n",
    "    MSE: Training: 29.577, Validation: 45.059\n",
    "    Decision Tree Model:\n",
    "\n",
    "    R2 Scores: Training: 0.834, Validation: 0.739\n",
    "    MSE: Training: 47.280, Validation: 73.447\n",
    "\n",
    "2. Model Selection\n",
    "Given the results, my preference leans towards the Gradient Boosting model. It not only achieves the highest training R2 score of 0.988 and an MSE of 3.379 but also excels in validation with an R2 score of 0.919 and an MSE of 22.783. These metrics underscore the model’s robust performance and its capability to generalize effectively from the training to the test set. The balance between the training and validation scores also indicates a commendable level of model fit, steering clear of overfitting and underfitting.\n",
    "\n",
    "3. Enhancing Accuracy of Tree-based Models\n",
    "Improving the accuracy of tree-based models can be approached in several ways. One effective strategy is parameter tuning:\n",
    "Max Depth Adjustment: Tweaking the max depth of the trees can significantly impact model accuracy. A deeper tree can capture more complex patterns, but it’s crucial to monitor for overfitting.\n",
    "Optimizing Tree Quantity and Feature Consideration in Random Forest: The n_estimators parameter, controlling the number of trees, can be increased for more robust results. Concurrently, adjusting the max_features parameter can infuse randomness into the model, enhancing diversity among the individual trees and boosting overall model performance.\n",
    "Through strategic manipulations of these parameters, the accuracy of tree-based models can be optimized to deliver more precise and reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb4132",
   "metadata": {},
   "source": [
    "1. My code was sourced from a combination of the lecture slides, practical examples in the Jupyter notebooks available on D2L, and interactive assistance from ChatGPT. Each source contributed to the overall development and refinement of my code.\n",
    "\n",
    "2. I began by reviewing the lecture slides to establish a solid theoretical foundation on linear regression.\n",
    "Subsequently, I delved into the Jupyter notebooks on D2L to witness the practical application and gather insights to reinforce my understanding.\n",
    "To further clarify complex concepts and enhance my understanding, I turned to ChatGPT, which provided detailed explanations and guidance.\n",
    "\n",
    "3. I used ChatGPT to get deeper insights into concepts like mean squared error, R2 score, and model fit. The prompts were focused on explanations and Python implementations of these concepts.\n",
    "Minor modifications were made to the generated code to tailor it to the specific requirements and dataset of the assignment, ensuring relevance and accuracy.\n",
    "\n",
    "4. Yes, I encountered challenges, particularly in understanding the specific Python commands and their outputs, like r2_score.\n",
    "ChatGPT played a crucial role in overcoming these challenges. The detailed explanations provided by the AI tool aided in demystifying the complex terms and concepts, granting me a clearer and more comprehensive understanding.\n",
    "\n",
    "Citations:\n",
    "\n",
    "OpenAI. (2023). ChatGPT API. Retrieved from https://www.openai.com/chatgpt-api\n",
    "\n",
    "Dawson, Leanne. (2023). ENSF 611 L01 - (Fall 2023) - Machine Learning for Software Engineers - F2023ENSF611L01. \n",
    "\n",
    "In Desire2Learn (Brightspace). https://d2l.ucalgary.ca/d2l/home/543310"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 2: Classification (17.5 marks)\n",
    "\n",
    "You have been asked to develop code that can help the user classify different wine samples. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (2 marks)\n",
    "\n",
    "The data used for this task can be downloaded from UCI: https://archive.ics.uci.edu/dataset/109/wine\n",
    "\n",
    "Use the pandas library to load the dataset. You must define the column headers if they are not included in the dataset \n",
    "\n",
    "You will need to split the dataset into feature matrix `X` and target vector `y`. Which column represents the target vector?\n",
    "\n",
    "Print the size and type of `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size and type of X:\n",
      "Size: 2314\n",
      "Type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "Size and type of y:\n",
      "Size: 178\n",
      "Type: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import wine dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "\n",
    "# Column headers\n",
    "columns = [\n",
    "    \"Class\",\n",
    "    \"Alcohol\",\n",
    "    \"Malic acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity of ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280/OD315 of diluted wines\",\n",
    "    \"Proline\"\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(url, header=None, names=columns)\n",
    "\n",
    "# Split the dataset into X and y\n",
    "X = df.drop(columns=[\"Class\"])\n",
    "y = df[\"Class\"]\n",
    "\n",
    "# Print the size and type of X and y\n",
    "print(\"Size and type of X:\")\n",
    "print(f\"Size: {X.size}\")\n",
    "print(f\"Type: {type(X)}\")\n",
    "print(\"\\nSize and type of y:\")\n",
    "print(f\"Size: {y.size}\")\n",
    "print(f\"Type: {type(y)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28af110",
   "metadata": {},
   "source": [
    "Print the first five rows of the dataset to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ea266921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
      "0      1    14.23        1.71  2.43               15.6        127   \n",
      "1      1    13.20        1.78  2.14               11.2        100   \n",
      "2      1    13.16        2.36  2.67               18.6        101   \n",
      "3      1    14.37        1.95  2.50               16.8        113   \n",
      "4      1    13.24        2.59  2.87               21.0        118   \n",
      "\n",
      "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
      "0           2.80        3.06                  0.28             2.29   \n",
      "1           2.65        2.76                  0.26             1.28   \n",
      "2           2.80        3.24                  0.30             2.81   \n",
      "3           3.85        3.49                  0.24             2.18   \n",
      "4           2.80        2.69                  0.39             1.82   \n",
      "\n",
      "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
      "0             5.64  1.04                          3.92     1065  \n",
      "1             4.38  1.05                          3.40     1050  \n",
      "2             5.68  1.03                          3.17     1185  \n",
      "3             7.80  0.86                          3.45     1480  \n",
      "4             4.32  1.04                          2.93      735  \n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fc8fe",
   "metadata": {},
   "source": [
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "97c6e9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class                           0\n",
       "Alcohol                         0\n",
       "Malic acid                      0\n",
       "Ash                             0\n",
       "Alcalinity of ash               0\n",
       "Magnesium                       0\n",
       "Total phenols                   0\n",
       "Flavanoids                      0\n",
       "Nonflavanoid phenols            0\n",
       "Proanthocyanins                 0\n",
       "Color intensity                 0\n",
       "Hue                             0\n",
       "OD280/OD315 of diluted wines    0\n",
       "Proline                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070956af",
   "metadata": {},
   "source": [
    "How many samples do we have of each type of wine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b37a6fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "2    71\n",
      "1    59\n",
      "3    48\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `SVC` and `DecisionTreeClassifier` from sklearn\n",
    "2. Instantiate models as `SVC()` and `DecisionTreeClassifier(max_depth = 3)`\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0b200404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=3, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, random_state=0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Instantiating and training the SVC model\n",
    "svc = SVC(random_state=0)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Instantiating and training the DecisionTreeClassifier\n",
    "tree_dt = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "tree_dt.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f5b314",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0870b0d2",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model \n",
    "\n",
    "Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9e0d5144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9943572562158348\n",
      "0.8940170940170941\n",
      "0.6804267324986774\n",
      "0.6766381766381766\n"
     ]
    }
   ],
   "source": [
    "# Using cross-validation to evaluate the performance of the Decision Tree model\n",
    "cv_results_dt = cross_validate(tree_dt, X_train, y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Using cross-validation to evaluate the performance of the Support Vector Machine (SVC) model\n",
    "cv_results_sv = cross_validate(svc, X_train, y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Calculating the average training accuracy for the Decision Tree model\n",
    "train_accuracy_dt = cv_results_dt['train_score'].mean()\n",
    "\n",
    "# Calculating the average validation accuracy for the Decision Tree model\n",
    "validation_accuracy_dt = cv_results_dt['test_score'].mean()\n",
    "\n",
    "# Calculating the average training accuracy for the SVC model\n",
    "train_accuracy_svm = cv_results_sv['train_score'].mean()\n",
    "\n",
    "# Calculating the average validation accuracy for the SVC model\n",
    "validation_accuracy_svm = cv_results_sv['test_score'].mean()\n",
    "\n",
    "# Printing the training and validation accuracies for the Decision Tree model\n",
    "print(train_accuracy_dt)\n",
    "print(validation_accuracy_dt)\n",
    "\n",
    "# Printing the training and validation accuracies for the SVC model\n",
    "print(train_accuracy_svm)\n",
    "print(validation_accuracy_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bbd83",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "#### Step 5.1: Compare Models\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Data Shape  Training Accuracy  Validation Accuracy\n",
      "DecisionTree  (133, 13)           0.994357             0.894017\n",
      "SVC           (133, 13)           0.680427             0.676638\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create an empty DataFrame with the desired columns\n",
    "results = pd.DataFrame(columns=[\"Data Shape\", \"Training Accuracy\", \"Validation Accuracy\"])\n",
    "\n",
    "models = {\"DecisionTree\": tree_dt, \"SVC\": svc}\n",
    "for model_name, model in models.items():\n",
    "    cv_results = cross_validate(model, X_train, y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "    \n",
    "    # Calculate average training and validation accuracy\n",
    "    train_accuracy = cv_results['train_score'].mean()\n",
    "    validation_accuracy = cv_results['test_score'].mean()\n",
    "    \n",
    "    # Determine data shape (number of rows and columns combined as a tuple)\n",
    "    data_shape = (X_train.shape[0], X_train.shape[1])\n",
    "    \n",
    "    # Add the results to the DataFrame\n",
    "    results.loc[model_name] = [data_shape, train_accuracy, validation_accuracy]\n",
    "\n",
    "    # Since \"Data Shape\" is now a tuple of integers, we don't need to convert its datatype\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e17878",
   "metadata": {},
   "source": [
    "#### Step 5.2: Visualize Classification Errors\n",
    "Which method gave the highest accuracy? Use this method to print the confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "44b091a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  2  0]\n",
      " [ 0 20  1]\n",
      " [ 0  0  8]]\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Implement best model\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Determine the best model based on validation accuracy\n",
    "best_model_name = results[\"Validation Accuracy\"].idxmax()\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Use the best model to predict the validation set and print confusion matrix\n",
    "y_pred = best_model.predict(X_val)\n",
    "mat = confusion_matrix(y_val, y_pred)\n",
    "print(mat)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "09d21b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: Print confusion matrix using a heatmap\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Visualize the confusion matrix using a heatmap\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"YlGnBu\", fmt=\"g\", \n",
    "            xticklabels=best_model.classes_, \n",
    "            yticklabels=best_model.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix for {best_model_name}')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5ef95947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.88      0.93        16\n",
      "           2       0.91      0.95      0.93        21\n",
      "           3       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.94      0.93        45\n",
      "weighted avg       0.94      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Print classification report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf319621",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do the training and validation accuracy change depending on the method used? Explain with values.\n",
    "1. What are two reasons why the support vector machines model did not work as well as the tree-based model?\n",
    "1. How many samples were incorrectly classified in step 5.2? \n",
    "1. In this case, is maximizing precision or recall more important? Why?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "\n",
    "1. With the results for one method. The validation accuracy for this method is 82% (from the classification report). Without the results from the support vector machines model, I cannot directly compare the training and validation accuracy of the two methods. However, based on our discussion, it seems that the tree-based model performed better.\n",
    "\n",
    "2. The SVM model might have been impacted due to the following reasons:\n",
    "\n",
    "    Parameter Sensitivity: SVMs are particularly responsive to the configuration of parameters like gamma and C. If not appropriately tuned, these can have a significant effect on the model's accuracy.\n",
    "    Data Scaling: The dataset attributes display a wide range of values, stretching from 0.1 to 1000. This substantial variation can influence the SVM's performance, as SVMs operate more efficiently when all features are roughly on the same scale.\n",
    "    Model Suitability: The nature of the data might be better captured by a non-linear model, such as a decision tree. While SVMs can introduce non-linearity with tools like polynomial features, this might not be necessary or optimal for our specific dataset.\n",
    "\n",
    "3. 3 samples were incorrectly classified, as derived from the confusion matrix.\n",
    "\n",
    "4. Looking at the classification report:\n",
    "Class 1 has high precision and recall.\n",
    "Class 2 has moderate precision and recall.\n",
    "Class 3 has the lowest precision.\n",
    "Deciding between precision and recall depends on the specific application. If it's more crucial to be sure of your predictions when you make them, you'd prioritize precision. If it's more vital to capture as many positive samples as possible, you'd prioritize recall.\n",
    "For wine classification, it may be more costly to falsely label a high-quality wine (class 1) as a lower quality (class 2 or 3), so you might prioritize precision for class 1. But if the aim is to ensure no low-quality wines (class 3) are labeled as high quality, you'd prioritize recall for class 3. Without specific business or application context, it's hard to definitively say which is more important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ff8ae",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173a932d",
   "metadata": {},
   "source": [
    "\n",
    "1. I referred primarily to the lecture notes for foundational understanding. Further, I delved into the Jupyter notebooks provided on D2L to witness practical code implementations.\n",
    "\n",
    "2. My strategy was sequential. I began by immersing myself in the lecture notes, particularly focusing on SVM and Decision Trees. This gave me a conceptual base. To put this theory into practice, I then turned to the Jupyter notebooks on D2L. And finally, when faced with uncertainties, I reached out to ChatGPT for guidance.\n",
    "\n",
    "3. For guidance, I approached ChatGPT with prompts like \"explain what is gamma in SVM model,\" \"distinguish between random forest and decision,\" and \"provide guidance on writing a loop to populate a pandas dataframe.\" These prompts not only bolstered my theoretical grasp but also smoothed the path for my Python implementation. Minor tweaks were necessary to align the AI's suggestions with my assignment's specifics.\n",
    "\n",
    "4. One of the pronounced challenges I encountered was wrapping my head around SVM's kernel functions. Specifically, understanding the intricacies of polynomial kernel and RBF/Gaussian kernel proved demanding. However, ChatGPT came to my aid, offering concise explanations that demystified these kernels, making my journey smoother.\n",
    "\n",
    "\n",
    "**Acknowledgments**:\n",
    "1. OpenAI. (2023). ChatGPT API. Accessed at: https://www.openai.com/chatgpt-api\n",
    "2. Dawson, Leanne. (2023). ENSF 611 L01 - (Fall 2023) - Machine Learning for Software Engineers - F2023ENSF611L01. Accessible at: https://d2l.ucalgary.ca/d2l/home/543310\n",
    "\n",
    "**In Summary**:\n",
    "My journey through this assignment was a blend of self-study, practical application, and leveraging AI-assisted insights. The rich interplay between these approaches ensured a wholesome learning experience, culminating in the successful completion of my task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e837da",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7358d",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*\n",
    "\n",
    "**Part 1 - Model Comparison and Accuracy Scores:**\n",
    "*Decision Tree vs. Random Forest vs. Gradient Boosting:*\n",
    "\n",
    "Using the decision tree with a max depth of 5, the model recorded a training score of 0.834 and a validation score of 0.739, suggesting a moderate performance. When the analysis transitioned to the random forest model, both training (0.897) and validation (0.841) scores exhibited a noticeable rise, showcasing the power of ensemble methods. The gradient boosting model topped the charts with a stellar training score of 0.988 and a validation score of 0.919, demonstrating the iterative strength of boosting algorithms.\n",
    "\n",
    "**Part 2 - SVC vs. Decision Tree Model Performance:**\n",
    "*Challenges with SVC Model:*\n",
    "\n",
    "The SVM model faced challenges, which might be attributed to two main factors: the lack of data scaling and the non-optimization of critical parameters such as gamma and C. Ensuring proper data scaling and optimizing these parameters might have led to better results.\n",
    "\n",
    "*Decision Tree's Consistent Performance:*\n",
    "\n",
    "The decision tree model, with its max depth set to 3, exhibited commendable performance. When compared with the SVC, its results were more consistent, making it the more reliable choice for this dataset.\n",
    "\n",
    "**Analysis and Insights:**\n",
    "*Impact of Model Choices and Parameters:*\n",
    "\n",
    "The side-by-side analysis of the decision tree, random forest, and gradient boosting demonstrated the varied outcomes depending on the model and its parameters. The gradient boosting model's exceptional performance highlighted the merits of iterative improvement and a strong algorithm.\n",
    "\n",
    "*Challenges with SVM Model:*\n",
    "\n",
    "The less-than-stellar performance of the SVM model underscores the importance of data preparation and parameter tuning. It's a testament to the fact that no single model is universally optimal; the choice and configuration of a model need to suit the specifics of the dataset in hand.\n",
    "\n",
    "*Balancing Model Complexity:*\n",
    "\n",
    "The comparison between SVC and the decision tree emphasizes the balance required in model complexity. While simplicity ensures understandability, it may not always yield the best results, as seen with the SVM model's performance. On the other hand, a well-configured decision tree balanced accuracy with model simplicity effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "\n",
    "Reflection:\n",
    "\n",
    "Throughout this assignment, I particularly enjoyed the process of comparing multiple machine learning models, as it provided insight into their strengths and limitations in varied scenarios. The challenge of optimizing parameters for each model, especially for the SVM, was intriguing, although at times it felt like navigating a maze. It was motivating to see how a well-configured decision tree can outshine even more complex models. However, understanding the nuances of why one model performed better than another, especially when they're derived from similar base algorithms, was both interesting and occasionally confusing.\n",
    "\n",
    "In addition to that, I appreciated the hands-on nature of this assignment. While I found the comparison between linear and non-linear models particularly insightful, the nuances of handling warnings in Python, such as the ConvergenceWarning, were initially a hurdle. Ensuring that the model converged effectively was a rigorous challenge but equally inspiring. The entire process prompted me to research more and seek out solutions, ultimately solidifying my problem-solving skills in machine learning. This whole experience underscored the importance of a thorough and systematic approach in data science endeavors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa21e53b",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (3 marks)\n",
    "\n",
    "Repeat Part 2 and compare the support vector machines model used to `LinearSVC(max_iter=5000)`. Does using `LinearSVC` improve the results? Why or why not?\n",
    "\n",
    "Is `LinearSVC` a good fit for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "30fea72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Size for Training</th>\n",
       "      <th>Data Size for Testing</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(133, 13)</td>\n",
       "      <td>(45, 13)</td>\n",
       "      <td>0.868295</td>\n",
       "      <td>0.849858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Data Size for Training Data Size for Testing  Training Accuracy  \\\n",
       "0              (133, 13)              (45, 13)           0.868295   \n",
       "\n",
       "   Validation Accuracy  \n",
       "0             0.849858  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "import pandas as pd\n",
    "\n",
    "# Suppress the specified warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "    warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    # Instantiate and train the model\n",
    "    LinearSVC_m = LinearSVC(max_iter=5000, random_state=0)\n",
    "    LinearSVC_m.fit(X_train, y_train)\n",
    "\n",
    "    # Cross validation\n",
    "    cv_results_m_score = cross_validate(LinearSVC_m, X_train, y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "    cv_results_train = cv_results_m_score[\"train_score\"].mean()\n",
    "    cv_result_test = cv_results_m_score[\"test_score\"].mean()\n",
    "\n",
    "    # Organize the data for the desired output format\n",
    "    output_data = {\n",
    "        'Data Size for Training': [X_train.shape],\n",
    "        'Data Size for Testing': [X_val.shape],\n",
    "        'Training Accuracy': [cv_results_train],\n",
    "        'Validation Accuracy': [cv_result_test]\n",
    "    }\n",
    "\n",
    "    results = pd.DataFrame(output_data)\n",
    "\n",
    "    display(results)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc68a4",
   "metadata": {},
   "source": [
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d371bef",
   "metadata": {},
   "source": [
    "Appropriateness of LinearSVC for the dataset:\n",
    "\n",
    "Improvement in Results: The results indicate a training accuracy of approximately 86.83% and a validation accuracy of about 84.98% for the LinearSVC model. If this accuracy is notably higher than the previous SVM model or any other models tested, it might suggest that a linear kernel is well-suited for this dataset.\n",
    "\n",
    "Speed: While we haven't been provided with timing results, one advantage of LinearSVC is its efficiency, especially for larger datasets. This is because LinearSVC is implemented based on the liblinear library, which is tailored for linear SVMs. In contrast, the generic SVC utilizes the libsvm library, which might be less efficient for linear SVM tasks.\n",
    "\n",
    "Dataset Characteristics: SVM models are traditionally effective for datasets where the classes aren't perfectly linearly separable. The relatively high accuracy achieved with LinearSVC might suggest that the dataset is approximately linearly separable or that a linear decision boundary is sufficient to capture the primary trends in the data.\n",
    "\n",
    "Good Fit or Not: From the presented results, LinearSVC seems to perform commendably on both the training and validation sets. If this performance is competitive with or surpasses other tested models—especially more intricate ones—while offering the benefits of speed and simplicity, then LinearSVC can be considered a good fit for this dataset. However, if other models achieve substantially better performance metrics or if there are concerns about overfitting or underfitting, further exploration of models or configurations might be warranted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
